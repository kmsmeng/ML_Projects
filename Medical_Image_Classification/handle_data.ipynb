{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91f5d248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from pydicom import dcmread\n",
    "from PIL import Image\n",
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "693d4c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Images: 26684\n",
      "Test Data Images: 3000\n"
     ]
    }
   ],
   "source": [
    "# Get all the train data samples and test data samples path as list\n",
    "\n",
    "data_file = Path('data/pneumonia_dataset')\n",
    "\n",
    "train_path = list(data_file.glob('stage_2_train_images/*.dcm'))\n",
    "test_path = list(data_file.glob('stage_2_test_images/*.dcm'))\n",
    "print(f'Train Data Images: {len(train_path)}')\n",
    "print(f'Test Data Images: {len(test_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12af7f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO Train Data Samples: 20000\n",
      "YOLO val Data Samples: 6684\n"
     ]
    }
   ],
   "source": [
    "# Split the total data of 26684 into train and val dataset for YOLOv8\n",
    "train_size = 20000\n",
    "\n",
    "\n",
    "yolo_train_path = train_path[0: 20000]\n",
    "val_path = train_path[20000:]\n",
    "\n",
    "print(f'YOLO Train Data Samples: {len(yolo_train_path)}')\n",
    "print(f'YOLO val Data Samples: {len(val_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53678393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patientId</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0004cfab-14fd-4e49-80ba-63a80b6bddd6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00313ee0-9eaa-42f4-b0ab-c148ed3241cd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00322d4d-1c29-4943-afc9-b6754be640eb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003d8fa0-6bf1-40ed-b54c-ac657f8495c5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00436515-870c-4b36-a041-de91049b9ab4</td>\n",
       "      <td>264.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>379.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30222</th>\n",
       "      <td>c1ec14ff-f6d7-4b38-b0cb-fe07041cbdc8</td>\n",
       "      <td>185.0</td>\n",
       "      <td>298.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>379.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30223</th>\n",
       "      <td>c1edf42b-5958-47ff-a1e7-4f23d99583ba</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30224</th>\n",
       "      <td>c1f6b555-2eb1-4231-98f6-50a963976431</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30225</th>\n",
       "      <td>c1f7889a-9ea9-4acb-b64c-b737c929599a</td>\n",
       "      <td>570.0</td>\n",
       "      <td>393.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>345.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30226</th>\n",
       "      <td>c1f7889a-9ea9-4acb-b64c-b737c929599a</td>\n",
       "      <td>233.0</td>\n",
       "      <td>424.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30227 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  patientId      x      y  width  height  \\\n",
       "0      0004cfab-14fd-4e49-80ba-63a80b6bddd6    NaN    NaN    NaN     NaN   \n",
       "1      00313ee0-9eaa-42f4-b0ab-c148ed3241cd    NaN    NaN    NaN     NaN   \n",
       "2      00322d4d-1c29-4943-afc9-b6754be640eb    NaN    NaN    NaN     NaN   \n",
       "3      003d8fa0-6bf1-40ed-b54c-ac657f8495c5    NaN    NaN    NaN     NaN   \n",
       "4      00436515-870c-4b36-a041-de91049b9ab4  264.0  152.0  213.0   379.0   \n",
       "...                                     ...    ...    ...    ...     ...   \n",
       "30222  c1ec14ff-f6d7-4b38-b0cb-fe07041cbdc8  185.0  298.0  228.0   379.0   \n",
       "30223  c1edf42b-5958-47ff-a1e7-4f23d99583ba    NaN    NaN    NaN     NaN   \n",
       "30224  c1f6b555-2eb1-4231-98f6-50a963976431    NaN    NaN    NaN     NaN   \n",
       "30225  c1f7889a-9ea9-4acb-b64c-b737c929599a  570.0  393.0  261.0   345.0   \n",
       "30226  c1f7889a-9ea9-4acb-b64c-b737c929599a  233.0  424.0  201.0   356.0   \n",
       "\n",
       "       Target  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           1  \n",
       "...       ...  \n",
       "30222       1  \n",
       "30223       0  \n",
       "30224       0  \n",
       "30225       1  \n",
       "30226       1  \n",
       "\n",
       "[30227 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# File path of the labels from the working data and convert the csv file into a pandas DataFrame\n",
    "\n",
    "label_file_path = 'data/pneumonia_dataset/stage_2_train_labels.csv'\n",
    "label_csv = pd.read_csv(label_file_path)\n",
    "label_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "371680e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Number of Data samples with pneumonia and without pneumonia\n",
    "\n",
    "counts = label_csv.drop_duplicates('patientId')['Target'].value_counts()\n",
    "\n",
    "# num_no_pneumonia = counts[0]\n",
    "# num_pneumonia = counts[1]\n",
    "\n",
    "data_class = {\n",
    "    'no pneumonia': int(counts.get(0, 0)),\n",
    "    'pneumonia': int(counts.get(1, 0))\n",
    "}\n",
    "\n",
    "total = sum(data_class.values())\n",
    "\n",
    "\n",
    "bars = plt.bar(data_class.keys(), data_class.values(), color=['cadetblue', 'salmon'])\n",
    "plt.bar_label(bars, labels=[f\"{value}\\n({value/total*100:.1f}%)\" for value in data_class.values()])\n",
    "plt.ylim(0, 1.2*max(data_class.values()))\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Pneumonia Dataset Class Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d3cbf7e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset.file_meta -------------------------------\n",
       "(0002,0000) File Meta Information Group Length  UL: 202\n",
       "(0002,0001) File Meta Information Version       OB: b'\\x00\\x01'\n",
       "(0002,0002) Media Storage SOP Class UID         UI: Secondary Capture Image Storage\n",
       "(0002,0003) Media Storage SOP Instance UID      UI: 1.2.276.0.7230010.3.1.4.8323329.28530.1517874485.775526\n",
       "(0002,0010) Transfer Syntax UID                 UI: JPEG Baseline (Process 1)\n",
       "(0002,0012) Implementation Class UID            UI: 1.2.276.0.7230010.3.0.3.6.0\n",
       "(0002,0013) Implementation Version Name         SH: 'OFFIS_DCMTK_360'\n",
       "-------------------------------------------------\n",
       "(0008,0005) Specific Character Set              CS: 'ISO_IR 100'\n",
       "(0008,0016) SOP Class UID                       UI: Secondary Capture Image Storage\n",
       "(0008,0018) SOP Instance UID                    UI: 1.2.276.0.7230010.3.1.4.8323329.28530.1517874485.775526\n",
       "(0008,0020) Study Date                          DA: '19010101'\n",
       "(0008,0030) Study Time                          TM: '000000.00'\n",
       "(0008,0050) Accession Number                    SH: ''\n",
       "(0008,0060) Modality                            CS: 'CR'\n",
       "(0008,0064) Conversion Type                     CS: 'WSD'\n",
       "(0008,0090) Referring Physician's Name          PN: ''\n",
       "(0008,103E) Series Description                  LO: 'view: PA'\n",
       "(0010,0010) Patient's Name                      PN: '0004cfab-14fd-4e49-80ba-63a80b6bddd6'\n",
       "(0010,0020) Patient ID                          LO: '0004cfab-14fd-4e49-80ba-63a80b6bddd6'\n",
       "(0010,0030) Patient's Birth Date                DA: ''\n",
       "(0010,0040) Patient's Sex                       CS: 'F'\n",
       "(0010,1010) Patient's Age                       AS: '51'\n",
       "(0018,0015) Body Part Examined                  CS: 'CHEST'\n",
       "(0018,5101) View Position                       CS: 'PA'\n",
       "(0020,000D) Study Instance UID                  UI: 1.2.276.0.7230010.3.1.2.8323329.28530.1517874485.775525\n",
       "(0020,000E) Series Instance UID                 UI: 1.2.276.0.7230010.3.1.3.8323329.28530.1517874485.775524\n",
       "(0020,0010) Study ID                            SH: ''\n",
       "(0020,0011) Series Number                       IS: '1'\n",
       "(0020,0013) Instance Number                     IS: '1'\n",
       "(0020,0020) Patient Orientation                 CS: ''\n",
       "(0028,0002) Samples per Pixel                   US: 1\n",
       "(0028,0004) Photometric Interpretation          CS: 'MONOCHROME2'\n",
       "(0028,0010) Rows                                US: 1024\n",
       "(0028,0011) Columns                             US: 1024\n",
       "(0028,0030) Pixel Spacing                       DS: [0.14300000000000002, 0.14300000000000002]\n",
       "(0028,0100) Bits Allocated                      US: 8\n",
       "(0028,0101) Bits Stored                         US: 8\n",
       "(0028,0102) High Bit                            US: 7\n",
       "(0028,0103) Pixel Representation                US: 0\n",
       "(0028,2110) Lossy Image Compression             CS: '01'\n",
       "(0028,2114) Lossy Image Compression Method      CS: 'ISO_10918_1'\n",
       "(7FE0,0010) Pixel Data                          OB: Array of 142006 elements"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if neumonia cases vary by metadata like (age, sex, image view)\n",
    "\n",
    "data = dcmread(train_path[0])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b262f551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcm_to_jpg(dcm_path, save_path):\n",
    "    '''\n",
    "    Convert a DICOM (.dcm) file to a JPEG (.jpg) file and save it.\n",
    "\n",
    "    This function reads a DICOM file from `dcm_path`, converts the pixel data\n",
    "    to an image, and saves the image as a JPEG file at `save_path`.\n",
    "\n",
    "    Args:\n",
    "        dcm_path (str): File path of the input DICOM file.\n",
    "        save_path (str): File path to save the output JPEG file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    save_path = Path(save_path)\n",
    "\n",
    "    if not save_path.is_dir():\n",
    "        dcm_data = dcmread(dcm_path)\n",
    "        dcm_pixel_array = dcm_data.pixel_array\n",
    "        image = Image.fromarray(dcm_pixel_array)\n",
    "        image.save(save_path, 'JPEG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91ec1939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create YOLO File format list\n",
    "\n",
    "yolo_file_format = ['YOLO_format_data', ['images', 'labels', 'data.yaml'], ['train', 'val']]\n",
    "\n",
    "# Create the YOLO file format directory\n",
    "for x in range(2):\n",
    "    for y in range(2):\n",
    "        form_path = os.path.join(yolo_file_format[0], yolo_file_format[1][x], yolo_file_format[2][y])\n",
    "        form_path = Path(form_path)\n",
    "        form_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # Create data.yaml file\n",
    "yaml_file = Path(yolo_file_format[0]) / yolo_file_format[1][2]\n",
    "\n",
    "# Content for the data.yaml file as required by the YOLO model\n",
    "yaml_content = {\n",
    "    'train': f'{yolo_file_format[1][0]}/{yolo_file_format[2][0]}',\n",
    "    'val': f'{yolo_file_format[1][0]}/{yolo_file_format[2][1]}',\n",
    "    'nc': 1,\n",
    "    'names': ['pneumonia']\n",
    "}\n",
    "\n",
    "# write the content into the data.yaml file\n",
    "with open(yaml_file, 'w') as file:\n",
    "    yaml.dump(yaml_content, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8393f50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the training images into the images direcotry\n",
    "\n",
    "train_image_name_list = []\n",
    "for x in range(len(yolo_train_path)):\n",
    "    img = f'img{x+1}.jpg'\n",
    "    train_image_name_list.append(img)\n",
    "\n",
    "train_yolo_img = Path('YOLO_format_data/images/train')\n",
    "\n",
    "\n",
    "\n",
    "for index, path in enumerate(yolo_train_path):\n",
    "    save_path = os.path.join(train_yolo_img, train_image_name_list[index])\n",
    "    dcm_to_jpg(dcm_path=path, save_path=save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30c74b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the val images into the val directory\n",
    "\n",
    "val_image_name_list = []\n",
    "cont = len(train_image_name_list)\n",
    "cont\n",
    "for x in range(len(val_path)):\n",
    "    img = f'img{cont+x+1}.jpg'\n",
    "    val_image_name_list.append(img)\n",
    "\n",
    "val_yolo_img = Path('YOLO_format_data/images/val')\n",
    "\n",
    "for index, path in enumerate(val_path):\n",
    "    save_path = os.path.join(val_yolo_img, val_image_name_list[index])\n",
    "    dcm_to_jpg(dcm_path=path, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd913384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patientId</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0004cfab-14fd-4e49-80ba-63a80b6bddd6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00313ee0-9eaa-42f4-b0ab-c148ed3241cd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00322d4d-1c29-4943-afc9-b6754be640eb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003d8fa0-6bf1-40ed-b54c-ac657f8495c5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00436515-870c-4b36-a041-de91049b9ab4</td>\n",
       "      <td>264.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>379.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30222</th>\n",
       "      <td>c1ec14ff-f6d7-4b38-b0cb-fe07041cbdc8</td>\n",
       "      <td>185.0</td>\n",
       "      <td>298.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>379.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30223</th>\n",
       "      <td>c1edf42b-5958-47ff-a1e7-4f23d99583ba</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30224</th>\n",
       "      <td>c1f6b555-2eb1-4231-98f6-50a963976431</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30225</th>\n",
       "      <td>c1f7889a-9ea9-4acb-b64c-b737c929599a</td>\n",
       "      <td>570.0</td>\n",
       "      <td>393.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>345.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30226</th>\n",
       "      <td>c1f7889a-9ea9-4acb-b64c-b737c929599a</td>\n",
       "      <td>233.0</td>\n",
       "      <td>424.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30227 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  patientId      x      y  width  height  \\\n",
       "0      0004cfab-14fd-4e49-80ba-63a80b6bddd6    NaN    NaN    NaN     NaN   \n",
       "1      00313ee0-9eaa-42f4-b0ab-c148ed3241cd    NaN    NaN    NaN     NaN   \n",
       "2      00322d4d-1c29-4943-afc9-b6754be640eb    NaN    NaN    NaN     NaN   \n",
       "3      003d8fa0-6bf1-40ed-b54c-ac657f8495c5    NaN    NaN    NaN     NaN   \n",
       "4      00436515-870c-4b36-a041-de91049b9ab4  264.0  152.0  213.0   379.0   \n",
       "...                                     ...    ...    ...    ...     ...   \n",
       "30222  c1ec14ff-f6d7-4b38-b0cb-fe07041cbdc8  185.0  298.0  228.0   379.0   \n",
       "30223  c1edf42b-5958-47ff-a1e7-4f23d99583ba    NaN    NaN    NaN     NaN   \n",
       "30224  c1f6b555-2eb1-4231-98f6-50a963976431    NaN    NaN    NaN     NaN   \n",
       "30225  c1f7889a-9ea9-4acb-b64c-b737c929599a  570.0  393.0  261.0   345.0   \n",
       "30226  c1f7889a-9ea9-4acb-b64c-b737c929599a  233.0  424.0  201.0   356.0   \n",
       "\n",
       "       Target  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           1  \n",
       "...       ...  \n",
       "30222       1  \n",
       "30223       0  \n",
       "30224       0  \n",
       "30225       1  \n",
       "30226       1  \n",
       "\n",
       "[30227 rows x 6 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# File path of the labels from the working data and convert the csv file into a pandas DataFrame\n",
    "\n",
    "label_file_path = 'data/pneumonia_dataset/stage_2_train_labels.csv'\n",
    "label_csv = pd.read_csv(label_file_path)\n",
    "label_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "955e2e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all the txt files names for train labels file\n",
    "\n",
    "train_image_txt_list = []\n",
    "\n",
    "for image_name in train_image_name_list:\n",
    "    image_name = Path(image_name)\n",
    "    name = image_name.stem\n",
    "    txt_fil_name = f'{name}.txt'\n",
    "    train_image_txt_list.append(txt_fil_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f3e96c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path to save the train labels file to\n",
    "labels_train = Path('YOLO_format_data/labels/train')\n",
    "\n",
    "\n",
    "# Iterate over each path from the list of training data path\n",
    "for index, path in enumerate(yolo_train_path):\n",
    "\n",
    "    # Patient Id of the instance of data \n",
    "    id = path.stem\n",
    "    # Get all the label data associated with the Id\n",
    "    bbox_data = label_csv[label_csv['patientId'] == id] \n",
    "\n",
    "    # final file path to save the instance of label data to\n",
    "    labels_train_txt = os.path.join(labels_train, train_image_txt_list[index])\n",
    "\n",
    "    # Create a  label file for each image data\n",
    "    with open(labels_train_txt, 'w') as f:\n",
    "        # write label into the file if the bbox for the image exists \n",
    "        if bbox_data.iloc[0, 5].item() == 1:\n",
    "            label_class_id = bbox_data.iloc[0, 5].item()\n",
    "            for x in range(len(bbox_data)):\n",
    "                x1 = bbox_data.iloc[x, 1].item() / 1024\n",
    "                y1 = bbox_data.iloc[x, 2].item() / 1024\n",
    "                width = bbox_data.iloc[x, 3].item() / 1024\n",
    "                height = bbox_data.iloc[x, 4].item() / 1024\n",
    "                f.write(f'{label_class_id-1} {x1} {y1} {width} {height}')\n",
    "                f.write('\\n')\n",
    "        # Write nth into the file if no bbox for the instance of data exist\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82c8ab7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all the txt files names for val labels file\n",
    "\n",
    "val_image_txt_list = []\n",
    "\n",
    "for image_name in val_image_name_list:\n",
    "    image_name = Path(image_name)\n",
    "    name = image_name.stem\n",
    "    txt_fil_name = f'{name}.txt'\n",
    "    val_image_txt_list.append(txt_fil_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1dd4ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_val = Path('YOLO_format_data/labels/val')\n",
    "\n",
    "x = 0\n",
    "for index, path in enumerate(val_path):\n",
    "    id = path.stem\n",
    "    bbox_data = label_csv[label_csv['patientId'] == id]\n",
    "\n",
    "    labels_train_txt = os.path.join(labels_val, val_image_txt_list[index])\n",
    "\n",
    "    with open(labels_train_txt, 'w') as f:\n",
    "        if bbox_data.iloc[0, 5].item() == 1:\n",
    "            label_class_id = bbox_data.iloc[0, 5].item()\n",
    "            for x in range(len(bbox_data)):\n",
    "                x1 = bbox_data.iloc[x, 1].item() / 1024\n",
    "                y1 = bbox_data.iloc[x, 2].item() / 1024\n",
    "                width = bbox_data.iloc[x, 3].item() / 1024\n",
    "                height = bbox_data.iloc[x, 4].item() / 1024\n",
    "                f.write(f'{label_class_id-1} {x1} {y1} {width} {height}')\n",
    "                f.write('\\n')\n",
    "        # Write nth into the file if no bbox for the instance of data exist\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba02949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cda3e18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cc3fc1f",
   "metadata": {},
   "source": [
    "# Workign with the data part is done...now fine tuning the model part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f28ddc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ffb981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "052ad12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bf5cc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.146 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.145  Python-3.13.1 torch-2.6.0+cu126 CUDA:0 (NVIDIA T400 4GB, 4096MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=YOLO_format_data/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=medical_imaging, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=c:\\Users\\w10195102\\Desktop\\PROJECTS\\ML_Projects\\runs\\detect\\medical_imaging, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "WARNING \u001b[34m\u001b[1mAMP: \u001b[0mchecks failed . AMP training on NVIDIA T400 4GB GPU may cause NaN losses or zero-mAP results, so AMP will be disabled during training.\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 11.14.2 MB/s, size: 59.3 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\w10195102\\Desktop\\PROJECTS\\ML_Projects\\Medical_Image_Classification\\YOLO_format_data\\labels\\train... 20000 images, 14917 backgrounds, 0 corrupt: 100%|██████████| 20000/20000 [00:18<00:00, 1102.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\w10195102\\Desktop\\PROJECTS\\ML_Projects\\Medical_Image_Classification\\YOLO_format_data\\labels\\train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 4.40.9 MB/s, size: 57.1 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\w10195102\\Desktop\\PROJECTS\\ML_Projects\\Medical_Image_Classification\\YOLO_format_data\\labels\\val... 6684 images, 5755 backgrounds, 0 corrupt: 100%|██████████| 6684/6684 [00:05<00:00, 1143.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\w10195102\\Desktop\\PROJECTS\\ML_Projects\\Medical_Image_Classification\\YOLO_format_data\\labels\\val.cache\n",
      "Plotting labels to c:\\Users\\w10195102\\Desktop\\PROJECTS\\ML_Projects\\runs\\detect\\medical_imaging\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mc:\\Users\\w10195102\\Desktop\\PROJECTS\\ML_Projects\\runs\\detect\\medical_imaging\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5      3.82G      2.372      4.695      2.326         14        640: 100%|██████████| 1250/1250 [21:50<00:00,  1.05s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 209/209 [02:59<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6684       1359      0.007       0.71     0.0475     0.0153\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5      3.84G      2.185      3.388       2.17         11        640: 100%|██████████| 1250/1250 [22:09<00:00,  1.06s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 209/209 [03:09<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6684       1359     0.0674      0.231     0.0525     0.0172\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5      3.84G        2.1      3.194        2.1         14        640:  78%|███████▊  | 973/1250 [16:28<04:41,  1.02s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_results = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mYOLO_format_data/data.yaml\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m640\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmedical_imaging\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w10195102\\Desktop\\PROJECTS\\ML_Projects\\.venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:797\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    794\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.trainer.model\n\u001b[32m    796\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer.hub_session = \u001b[38;5;28mself\u001b[39m.session  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m797\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[32m    799\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w10195102\\Desktop\\PROJECTS\\ML_Projects\\.venv\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:227\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    224\u001b[39m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w10195102\\Desktop\\PROJECTS\\ML_Projects\\.venv\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:435\u001b[39m, in \u001b[36mBaseTrainer._do_train\u001b[39m\u001b[34m(self, world_size)\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n\u001b[32m    434\u001b[39m     loss_length = \u001b[38;5;28mself\u001b[39m.tloss.shape[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.tloss.shape) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m     \u001b[43mpbar\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_description\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m%11s\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m%11.4g\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m        \u001b[49m\u001b[43m%\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43m+\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[32;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[33;43m.3g\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43mG\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# (GB) GPU memory util\u001b[39;49;00m\n\u001b[32m    440\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloss_length\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# losses\u001b[39;49;00m\n\u001b[32m    441\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# batch size, i.e. 8\u001b[39;49;00m\n\u001b[32m    442\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# imgsz, i.e 640\u001b[39;49;00m\n\u001b[32m    443\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m     \u001b[38;5;28mself\u001b[39m.run_callbacks(\u001b[33m\"\u001b[39m\u001b[33mon_batch_end\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    446\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.plots \u001b[38;5;129;01mand\u001b[39;00m ni \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.plot_idx:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w10195102\\Desktop\\PROJECTS\\ML_Projects\\.venv\\Lib\\site-packages\\tqdm\\std.py:1382\u001b[39m, in \u001b[36mtqdm.set_description\u001b[39m\u001b[34m(self, desc, refresh)\u001b[39m\n\u001b[32m   1379\u001b[39m     \u001b[38;5;28mself\u001b[39m._ema_miniters = EMA(\u001b[38;5;28mself\u001b[39m.smoothing)\n\u001b[32m   1380\u001b[39m     \u001b[38;5;28mself\u001b[39m.refresh()\n\u001b[32m-> \u001b[39m\u001b[32m1382\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mset_description\u001b[39m(\u001b[38;5;28mself\u001b[39m, desc=\u001b[38;5;28;01mNone\u001b[39;00m, refresh=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1383\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1384\u001b[39m \u001b[33;03m    Set/modify description of the progress bar.\u001b[39;00m\n\u001b[32m   1385\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1390\u001b[39m \u001b[33;03m        Forces refresh [default: True].\u001b[39;00m\n\u001b[32m   1391\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1392\u001b[39m     \u001b[38;5;28mself\u001b[39m.desc = desc + \u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m desc \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_results = model.train(\n",
    "    data='YOLO_format_data/data.yaml',\n",
    "    epochs=5,\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    name='medical_imaging'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84df87a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "# len(yolo_train_path)\n",
    "# x = dcmread(yolo_train_path[3])\n",
    "# shape = x.pixel_array.shape\n",
    "# for x in shape:\n",
    "#     print(x)\n",
    "\n",
    "\n",
    "# for index, path in enumerate(yolo_train_path):\n",
    "#     data = dcmread(path)\n",
    "#     image_shape = data.pixel_array.shape\n",
    "#     for x in image_shape:\n",
    "#         if x != 1024:\n",
    "#             print(index)\n",
    "#             break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff027d6",
   "metadata": {},
   "source": [
    "Great — since you're training YOLOv8 in a Python notebook using the Ultralytics library, here's exactly how to safely stop training and resume it later.\n",
    "\n",
    "✅ How to Stop Training Safely\n",
    "You can interrupt the notebook kernel (or click the stop button), and YOLOv8 will automatically save the last checkpoint.\n",
    "\n",
    "By default, it saves to:\n",
    "\n",
    "swift\n",
    "Copy\n",
    "Edit\n",
    "runs/detect/train/weights/last.pt\n",
    "✅ How to Resume Training Later\n",
    "In a new cell, just run:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load from the last saved checkpoint\n",
    "model = YOLO('runs/detect/train/weights/last.pt')\n",
    "\n",
    "# Resume training\n",
    "model.train(resume=True)\n",
    "This resumes:\n",
    "\n",
    "Optimizer state\n",
    "\n",
    "Epoch count\n",
    "\n",
    "Best weights\n",
    "\n",
    "All hyperparameters\n",
    "\n",
    "🔁 Optional: Resume with Adjustments\n",
    "Want to change something (e.g., number of total epochs)?\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "model.train(resume=True, epochs=100)\n",
    "YOLO will pick up where it left off and continue up to 100 total epochs.\n",
    "\n",
    "📁 Where Your Checkpoints Are Saved\n",
    "Each training run creates a directory like:\n",
    "\n",
    "bash\n",
    "Copy\n",
    "Edit\n",
    "runs/detect/train/  # or train2, train3, etc.\n",
    "Inside that, you'll see:\n",
    "\n",
    "weights/last.pt – the latest checkpoint\n",
    "\n",
    "weights/best.pt – the best performing model so far\n",
    "\n",
    "results.csv – your training logs\n",
    "\n",
    "💡 Pro Tip: Set a Custom Save Path\n",
    "To avoid confusion across multiple runs:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "model.train(data='data.yaml', epochs=100, name='rsna_pneumonia_run1')\n",
    "This saves to:\n",
    "\n",
    "bash\n",
    "Copy\n",
    "Edit\n",
    "runs/detect/rsna_pneumonia_run1/\n",
    "Let me know if you'd like to auto-resume from the best model or evaluate your current checkpoint!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e2c355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd6ebfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc277279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training the mdoel form where it left of last time\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('runs/detect/medical_imaging_v22/weights/last.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5448e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.146 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.145  Python-3.13.1 torch-2.6.0+cu126 CUDA:0 (NVIDIA T400 4GB, 4096MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=YOLO_format_data/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=runs\\detect\\medical_imaging_v22\\weights\\last.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=medical_imaging_v22, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/detect, rect=False, resume=runs\\detect\\medical_imaging_v22\\weights\\last.pt, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\detect\\medical_imaging_v22, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.0, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "WARNING \u001b[34m\u001b[1mAMP: \u001b[0mchecks failed . AMP training on NVIDIA T400 4GB GPU may cause NaN losses or zero-mAP results, so AMP will be disabled during training.\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 302.9173.9 MB/s, size: 59.3 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\w10195102\\Desktop\\PROJECTS\\ML_Projects\\Medical_Image_Classification\\YOLO_format_data\\labels\\train.cache... 20000 images, 14917 backgrounds, 0 corrupt: 100%|██████████| 20000/20000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 212.5133.7 MB/s, size: 57.1 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\w10195102\\Desktop\\PROJECTS\\ML_Projects\\Medical_Image_Classification\\YOLO_format_data\\labels\\val.cache... 6684 images, 5755 backgrounds, 0 corrupt: 100%|██████████| 6684/6684 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\medical_imaging_v22\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Resuming training runs\\detect\\medical_imaging_v22\\weights\\last.pt from epoch 2 to 5 total epochs\n",
      "Closing dataloader mosaic\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\medical_imaging_v22\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5      3.85G      1.941       2.92      1.999         14        640: 100%|██████████| 1250/1250 [21:24<00:00,  1.03s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 209/209 [03:00<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6684       1359      0.161      0.194      0.101     0.0321\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5      3.84G      1.961      2.937      2.002         11        640: 100%|██████████| 1250/1250 [21:56<00:00,  1.05s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 209/209 [03:07<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6684       1359      0.236      0.162      0.122     0.0422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5      3.83G      1.931      2.872      1.981         28        640: 100%|██████████| 1250/1250 [21:24<00:00,  1.03s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 209/209 [03:08<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6684       1359      0.173      0.283      0.114     0.0394\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5      3.84G      1.878      2.784      1.939          8        640: 100%|██████████| 1250/1250 [21:07<00:00,  1.01s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 209/209 [03:10<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6684       1359      0.295      0.241      0.206     0.0731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4 epochs completed in 1.640 hours.\n",
      "Optimizer stripped from runs\\detect\\medical_imaging_v22\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from runs\\detect\\medical_imaging_v22\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating runs\\detect\\medical_imaging_v22\\weights\\best.pt...\n",
      "Ultralytics 8.3.145  Python-3.13.1 torch-2.6.0+cu126 CUDA:0 (NVIDIA T400 4GB, 4096MiB)\n",
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 209/209 [01:46<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6684       1359      0.296      0.241      0.206      0.073\n",
      "Speed: 0.5ms preprocess, 14.0ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\medical_imaging_v22\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
       "\n",
       "ap_class_index: array([0])\n",
       "box: ultralytics.utils.metrics.Metric object\n",
       "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x000001E0815BE060>\n",
       "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
       "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1,           1,           1,           1,        0.75,     0.69608,     0.69608,     0.69608,     0.69608,     0.69608,     0.69608,     0.69608,     0.69608,     0.69608,     0.69608,     0.69608,     0.69608,     0.69608,     0.69608,     0.69608,     0.69608,\n",
       "            0.69608,     0.69608,     0.69608,     0.69608,     0.69608,     0.69608,     0.69608,     0.69608,     0.69608,     0.69608,     0.69608,     0.69608,     0.69608,     0.69608,     0.69608,     0.69608,     0.69608,     0.69608,     0.69608,     0.69608,     0.69608,     0.69608,     0.69608,\n",
       "            0.69608,     0.69608,     0.69608,     0.69608,     0.69608,     0.69608,     0.69608,     0.68142,     0.68142,     0.68142,     0.68142,     0.67241,     0.67227,     0.65323,     0.63433,     0.63433,     0.63433,     0.62676,     0.62676,     0.62676,     0.62069,     0.61589,     0.61589,\n",
       "            0.61438,     0.60625,     0.60625,     0.60606,     0.60606,     0.60335,     0.60335,     0.60335,     0.60335,     0.60335,     0.60335,     0.60215,     0.60215,     0.60215,     0.60106,     0.59184,     0.59184,     0.58794,     0.58621,     0.58537,     0.58333,     0.58333,     0.58333,\n",
       "            0.58333,     0.56696,     0.55895,     0.55556,     0.54545,     0.54545,     0.54331,     0.54331,     0.54331,     0.54331,     0.54086,     0.54023,     0.53137,     0.53137,     0.53114,     0.53091,     0.53069,     0.52465,     0.52083,     0.51877,     0.51689,     0.50993,     0.50161,\n",
       "            0.49842,     0.49842,     0.48415,     0.48415,     0.48415,     0.48415,     0.48415,     0.48415,     0.48415,     0.48286,     0.47945,     0.47945,     0.47945,     0.47945,     0.47568,      0.4734,     0.46615,      0.4653,      0.4653,     0.46305,     0.46305,     0.46305,     0.46305,\n",
       "            0.46305,     0.46081,     0.46081,     0.46081,     0.46081,     0.45455,     0.44796,     0.44796,     0.44619,     0.44543,     0.44396,      0.4428,      0.4428,      0.4428,      0.4428,      0.4428,     0.44235,     0.44235,     0.43945,     0.43945,     0.43945,     0.43945,     0.43945,\n",
       "            0.43945,     0.43945,     0.43945,     0.43945,     0.43945,     0.43822,     0.43678,     0.43536,     0.42596,     0.42596,     0.42596,     0.41786,      0.4177,     0.41368,     0.41368,     0.41368,     0.41368,     0.41256,     0.40803,     0.40362,     0.40357,     0.40357,     0.40357,\n",
       "            0.39778,     0.39286,     0.39258,     0.39231,     0.39084,     0.38855,     0.38696,     0.38696,     0.38696,     0.38696,     0.38696,     0.38696,     0.38696,     0.38617,     0.37975,     0.37725,     0.37552,     0.37313,     0.37313,     0.37281,     0.37067,     0.36566,     0.36399,\n",
       "            0.36015,     0.35556,     0.35556,     0.35556,     0.35556,     0.35373,     0.35323,     0.34721,     0.34062,     0.34062,     0.34062,     0.33941,     0.33941,      0.3367,     0.33594,       0.329,       0.329,     0.32495,     0.32495,     0.32495,     0.32495,     0.32432,     0.32432,\n",
       "            0.31939,     0.31658,     0.31568,     0.31299,     0.31275,     0.30361,      0.2976,     0.29752,     0.29752,     0.29646,     0.29646,     0.29576,     0.29247,     0.29247,     0.29247,     0.29247,     0.29207,      0.2901,      0.2901,      0.2901,     0.28607,     0.28161,     0.28161,\n",
       "            0.27913,     0.27903,     0.27827,     0.27655,     0.27646,     0.27064,     0.27064,     0.27064,     0.26347,     0.26347,     0.26246,     0.26182,     0.25913,     0.25913,     0.25724,     0.25611,     0.25611,     0.25609,     0.25519,     0.25051,     0.25051,     0.24933,     0.24917,\n",
       "            0.24786,      0.2477,     0.24534,     0.24534,     0.24534,     0.24317,     0.24258,     0.24153,     0.24082,      0.2404,     0.23888,     0.23888,     0.23888,     0.23667,     0.23667,     0.23571,     0.23279,     0.23279,     0.23206,     0.22968,     0.22968,     0.22862,     0.22862,\n",
       "            0.22862,     0.22862,     0.22833,     0.22833,     0.22767,      0.2274,      0.2274,     0.22541,     0.22541,     0.22541,     0.22541,     0.22541,     0.22541,     0.22439,     0.22286,     0.22102,     0.22102,     0.21998,     0.21998,     0.21998,     0.21998,     0.21998,     0.21709,\n",
       "            0.21709,     0.21709,     0.21709,     0.21667,     0.21667,     0.21667,     0.21667,     0.21655,     0.21655,     0.21651,     0.21585,     0.21574,     0.21574,     0.21509,     0.21455,     0.21455,     0.21455,     0.21251,     0.21151,     0.21149,     0.20961,     0.20961,     0.20846,\n",
       "            0.20741,     0.20722,     0.20378,     0.20378,     0.20351,      0.2022,      0.2022,     0.20059,     0.20025,     0.20025,     0.20008,     0.19927,     0.19927,     0.19927,      0.1987,     0.19839,     0.19839,     0.19839,     0.19808,     0.19651,     0.19645,     0.19645,     0.19516,\n",
       "            0.19434,     0.19303,     0.19303,     0.19196,     0.19134,     0.19134,     0.19128,      0.1894,     0.18937,     0.18937,     0.18871,     0.18862,     0.18862,     0.18859,     0.18836,     0.18757,     0.18757,     0.18757,     0.18715,     0.18715,     0.18715,     0.18715,     0.18691,\n",
       "            0.18669,     0.18583,     0.18583,     0.18566,      0.1852,     0.18516,     0.18497,     0.18497,     0.18497,     0.18481,     0.18461,     0.18461,     0.18407,     0.18407,     0.18385,     0.18323,     0.18224,     0.18217,     0.18217,     0.18217,     0.18217,     0.18214,     0.18135,\n",
       "            0.17958,     0.17958,     0.17958,     0.17958,     0.17944,     0.17943,     0.17943,     0.17943,     0.17902,     0.17822,      0.1777,     0.17652,     0.17528,     0.17432,     0.17027,     0.16971,     0.16971,     0.16887,     0.16838,     0.16789,     0.16789,     0.16789,     0.16789,\n",
       "            0.16751,     0.16676,     0.16639,     0.16639,     0.16506,     0.16435,     0.16431,     0.16263,     0.16242,       0.162,     0.16185,     0.16185,     0.16177,     0.16085,     0.15969,     0.15947,     0.15947,     0.15947,     0.15947,     0.15944,     0.15924,     0.15907,     0.15907,\n",
       "             0.1586,      0.1586,      0.1586,     0.15841,     0.15841,     0.15841,     0.15841,     0.15841,     0.15841,     0.15841,     0.15841,     0.15841,     0.15841,     0.15831,     0.15458,     0.15442,     0.15421,     0.15408,      0.1535,      0.1528,     0.15218,     0.15218,     0.15036,\n",
       "            0.14992,     0.14984,     0.14972,      0.1496,     0.14832,     0.14729,     0.14621,     0.14617,     0.14617,     0.14575,     0.14564,     0.14473,     0.14423,     0.14389,     0.14389,     0.14389,     0.14256,     0.14143,     0.14141,     0.14094,     0.14014,     0.13977,     0.13972,\n",
       "            0.13945,     0.13915,     0.13818,     0.13739,     0.13737,     0.13636,     0.13627,     0.13612,     0.13425,     0.13174,     0.13082,      0.1308,      0.1308,     0.12992,     0.12908,     0.12856,     0.12815,     0.12783,     0.12734,     0.12709,      0.1263,     0.12617,       0.125,\n",
       "              0.125,       0.125,       0.125,     0.12366,     0.12339,     0.12337,     0.12269,     0.12193,     0.12193,     0.12114,     0.12114,     0.12065,     0.12058,     0.11893,     0.11893,     0.11867,     0.11867,     0.11867,     0.11779,     0.11733,     0.11731,     0.11652,     0.11652,\n",
       "            0.11652,     0.11643,     0.11643,     0.11624,     0.11555,     0.11466,     0.11326,     0.11304,     0.11265,     0.11234,     0.11058,      0.1105,     0.11042,     0.11022,     0.11022,     0.11022,     0.11022,     0.10908,     0.10847,     0.10775,     0.10698,     0.10698,     0.10698,\n",
       "            0.10698,     0.10675,     0.10675,     0.10637,     0.10606,     0.10531,     0.10531,     0.10531,     0.10507,     0.10494,     0.10492,     0.10492,     0.10463,     0.10463,     0.10395,     0.10385,     0.10349,     0.10254,     0.10254,     0.10188,      0.1012,     0.10086,     0.10086,\n",
       "            0.10002,    0.099682,    0.098779,    0.098308,    0.098308,    0.098159,    0.097347,    0.097319,    0.097242,    0.096983,    0.096571,    0.096485,    0.096344,    0.096174,     0.09532,      0.0945,    0.094494,    0.094494,    0.094494,    0.094494,    0.094494,    0.094192,    0.094192,\n",
       "           0.094192,    0.093757,    0.093502,    0.093502,    0.093319,    0.093319,    0.092869,    0.092373,     0.09216,     0.09203,    0.091931,    0.091878,    0.091878,    0.091536,    0.091361,    0.091321,    0.090313,    0.090201,    0.089552,    0.088131,    0.086465,    0.085745,     0.08528,\n",
       "           0.083619,    0.083207,    0.083207,    0.082958,    0.082958,    0.082958,    0.082241,    0.081878,    0.081736,     0.08118,    0.080689,    0.080353,     0.07947,    0.078982,    0.078666,    0.078666,    0.078666,    0.078658,    0.078402,     0.07821,     0.07783,    0.077258,    0.076142,\n",
       "           0.075585,    0.074545,    0.073927,    0.073653,    0.073653,    0.073253,    0.072855,    0.072645,    0.072078,    0.072078,    0.072026,    0.071734,    0.071628,    0.070109,    0.070068,    0.069788,    0.069788,    0.069667,    0.069667,    0.068235,     0.06799,    0.067865,    0.067521,\n",
       "           0.066932,    0.066932,    0.066766,    0.066747,    0.066451,    0.065818,    0.065496,    0.064492,    0.064424,    0.063614,    0.063614,    0.062991,    0.062693,     0.06199,    0.061536,     0.06148,     0.06148,    0.061293,    0.059196,    0.059144,    0.058741,    0.058338,    0.058139,\n",
       "           0.057838,    0.057682,     0.05734,    0.056015,    0.055854,    0.055764,    0.055764,    0.055726,    0.055543,    0.055172,    0.054853,    0.054853,    0.054307,    0.054307,    0.054076,    0.053012,     0.05266,    0.052233,    0.051964,    0.051756,     0.05108,    0.051079,    0.050883,\n",
       "           0.050842,      0.0501,    0.049814,    0.049491,    0.049401,    0.049346,    0.049346,    0.049346,    0.048397,    0.047975,     0.04772,    0.047384,    0.047259,      0.0471,    0.046722,    0.046722,    0.046549,    0.045964,    0.045964,    0.045681,    0.045338,    0.045315,     0.04482,\n",
       "            0.04482,    0.044754,    0.044219,    0.044219,    0.044219,    0.043941,    0.043184,    0.042871,    0.042796,    0.042786,     0.04266,    0.041596,    0.041525,    0.041056,    0.040862,    0.040749,    0.040531,    0.040531,    0.040191,    0.040191,    0.039815,    0.039313,    0.038708,\n",
       "             0.0386,    0.037879,    0.037841,    0.037591,    0.037008,    0.036726,    0.036726,    0.036425,    0.036425,    0.036151,    0.036016,    0.035921,    0.035165,    0.034951,    0.034769,    0.034754,    0.034603,    0.034554,    0.033991,    0.033943,    0.033197,    0.033146,     0.03306,\n",
       "           0.032812,    0.032655,    0.032526,    0.032369,    0.032353,    0.032296,    0.032221,    0.032221,    0.032202,    0.031876,    0.031775,    0.031421,    0.031221,    0.031214,    0.030215,    0.030214,    0.030153,    0.029584,    0.029477,    0.029401,    0.029111,     0.02904,    0.028804,\n",
       "           0.028679,    0.028645,    0.028645,    0.028593,    0.028593,    0.028447,    0.028418,    0.028399,    0.028161,    0.028161,    0.028014,    0.027467,    0.027318,    0.027318,    0.026918,    0.026854,    0.026297,    0.026049,    0.025925,    0.025572,    0.025408,    0.025272,    0.025154,\n",
       "           0.025077,    0.024757,    0.024627,    0.024398,    0.024308,    0.024258,    0.024249,    0.023728,    0.023728,    0.023678,    0.023645,    0.023439,    0.023343,    0.023265,    0.022811,    0.022811,    0.022788,    0.022734,    0.022621,    0.022619,    0.022557,    0.022482,    0.022387,\n",
       "           0.022021,     0.02197,     0.02197,    0.021885,    0.021782,    0.021621,    0.020949,    0.020794,    0.020511,    0.020261,     0.02024,    0.020068,    0.019826,    0.019713,    0.019366,    0.019292,    0.019252,    0.019223,    0.019223,     0.01917,    0.018629,     0.01844,     0.01844,\n",
       "            0.01778,    0.017682,    0.017654,    0.017388,    0.017372,    0.016935,    0.016821,    0.016608,    0.016487,    0.016463,    0.016302,    0.016228,    0.015627,    0.015599,    0.015539,    0.015529,    0.015398,     0.01521,    0.014931,    0.014724,     0.01446,    0.014448,    0.014078,\n",
       "           0.014055,    0.013855,    0.013695,    0.013537,     0.01335,    0.013115,    0.012862,    0.012693,    0.012629,     0.01251,    0.012307,    0.012267,    0.012091,    0.011814,    0.011804,    0.011693,     0.01157,    0.011545,    0.011224,    0.011028,      0.0108,    0.010585,    0.010288,\n",
       "             0.0102,   0.0099637,   0.0098697,   0.0098176,   0.0095489,   0.0094203,   0.0093406,   0.0092638,   0.0092132,   0.0085773,   0.0082627,   0.0081969,   0.0077863,   0.0075795,   0.0075713,   0.0073696,   0.0072329,   0.0071985,   0.0070606,   0.0069692,    0.006665,   0.0066125,   0.0061848,\n",
       "          0.0059956,   0.0059517,   0.0057358,   0.0056277,   0.0055904,   0.0055087,    0.005365,   0.0052508,   0.0050488,   0.0048469,   0.0046449,   0.0044429,    0.004241,    0.004039,   0.0038371,   0.0036351,   0.0034332,   0.0032312,   0.0030293,   0.0028273,   0.0026254,   0.0024234,   0.0022215,\n",
       "          0.0020195,   0.0018176,   0.0016156,   0.0014137,   0.0012117,   0.0010098,  0.00080781,  0.00060586,   0.0004039,  0.00020195,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.01063,    0.010638,    0.016867,    0.022225,    0.026955,    0.031313,    0.035381,    0.039236,     0.04298,    0.046307,    0.049913,    0.053538,    0.056595,    0.060127,    0.063094,    0.066586,    0.070026,     0.07342,    0.076886,    0.080246,    0.083535,    0.086729,    0.090099,\n",
       "           0.093242,    0.096633,     0.10025,     0.10341,     0.10682,     0.11037,     0.11407,     0.11762,      0.1212,     0.12399,     0.12766,     0.13121,     0.13453,     0.13839,     0.14096,     0.14498,     0.14795,      0.1522,     0.15648,     0.15985,      0.1622,      0.1638,     0.16738,\n",
       "            0.16994,     0.17283,     0.17629,     0.17805,     0.18023,     0.18364,     0.18556,     0.18859,     0.19178,     0.19379,     0.19659,     0.19844,     0.20113,     0.20294,     0.20584,     0.20838,     0.21127,     0.21515,     0.21718,     0.21917,     0.22178,     0.22399,     0.22562,\n",
       "            0.22741,     0.22914,     0.23135,     0.23301,     0.23545,     0.23597,     0.23563,      0.2357,     0.23692,     0.23794,     0.23961,     0.24154,     0.24236,     0.24353,     0.24571,     0.24836,     0.25046,      0.2512,      0.2521,     0.25193,     0.25271,     0.25286,     0.25225,\n",
       "            0.25231,     0.25271,     0.25223,     0.25259,     0.25205,     0.25179,     0.25305,     0.25339,     0.25422,     0.25539,     0.25611,     0.25602,      0.2555,     0.25592,     0.25693,     0.25654,     0.25913,      0.2595,     0.26001,     0.26087,     0.26191,     0.26092,     0.26182,\n",
       "            0.26085,     0.25966,     0.25988,     0.25989,      0.2591,     0.25989,     0.26067,     0.26121,     0.26041,      0.2593,     0.26031,     0.26019,     0.25955,     0.25865,     0.25923,     0.25953,     0.26004,     0.26052,     0.26056,     0.26053,     0.26081,     0.26045,     0.26081,\n",
       "            0.26071,     0.26179,      0.2604,     0.26122,     0.26134,       0.261,     0.26103,     0.26158,      0.2621,     0.26262,     0.26236,     0.26212,     0.26246,     0.26236,     0.26292,     0.26248,     0.26246,     0.26321,     0.26372,     0.26543,     0.26339,     0.26439,     0.26633,\n",
       "            0.26599,     0.26597,     0.26574,     0.26503,     0.26572,     0.26517,     0.26632,     0.26675,     0.26811,     0.26757,     0.26728,     0.26679,     0.26452,     0.26554,     0.26527,     0.26448,     0.26466,     0.26403,     0.26335,     0.26426,      0.2649,     0.26534,     0.26628,\n",
       "            0.26765,     0.26708,      0.2676,     0.26735,     0.26675,     0.26714,     0.26823,     0.26809,     0.26733,     0.26488,     0.26491,     0.26537,     0.26601,     0.26494,     0.26582,     0.26614,     0.26695,     0.26531,     0.26637,      0.2654,     0.26295,     0.26434,     0.26518,\n",
       "            0.26558,     0.26503,      0.2635,     0.26204,     0.26261,     0.26306,     0.26344,     0.26248,     0.26252,      0.2636,     0.26283,      0.2608,     0.26111,     0.26095,     0.26008,     0.26072,     0.26126,     0.26101,      0.2606,     0.25617,     0.25407,     0.25481,     0.25412,\n",
       "             0.2536,     0.25251,      0.2519,     0.25146,     0.25176,     0.25188,     0.25028,     0.24857,     0.24892,     0.24897,     0.24931,     0.24893,     0.24613,     0.24515,     0.24466,     0.24381,     0.24341,     0.24361,     0.24161,     0.24255,     0.24272,     0.24097,     0.24023,\n",
       "            0.23601,     0.23565,     0.23357,     0.23057,     0.22932,     0.22928,     0.22965,     0.22799,     0.22521,     0.22447,     0.22338,     0.22102,     0.21966,     0.21797,     0.21736,      0.2173,     0.21763,     0.21699,     0.21296,      0.2128,       0.213,     0.20989,     0.20647,\n",
       "            0.20695,     0.20562,     0.20441,     0.20506,     0.20355,     0.20177,     0.20112,     0.19814,     0.19673,     0.19567,     0.19255,     0.19111,     0.18994,       0.187,     0.18738,     0.18802,     0.18718,     0.18582,     0.18493,     0.18443,       0.184,     0.18212,     0.18028,\n",
       "            0.17952,     0.17767,     0.17508,     0.17372,       0.171,     0.17109,     0.16683,     0.16615,      0.1647,     0.16433,     0.16286,     0.16206,      0.1606,      0.1596,      0.1595,     0.15751,     0.15325,      0.1523,     0.15015,     0.14878,     0.14653,     0.14025,     0.13691,\n",
       "            0.13515,     0.13462,     0.13045,      0.1271,     0.12432,     0.12209,     0.11955,     0.11963,     0.11845,     0.11358,     0.11102,     0.10901,     0.10916,     0.10823,     0.10574,     0.10354,     0.09829,    0.097788,    0.095565,    0.092028,    0.087102,    0.081251,    0.076803,\n",
       "           0.072257,    0.069627,    0.064929,    0.064368,    0.063073,    0.063109,    0.058275,    0.054576,    0.052299,    0.049519,    0.048246,    0.047654,    0.045488,    0.043656,    0.042744,     0.03799,    0.035514,    0.033914,    0.033077,    0.030151,    0.028764,    0.027347,    0.024045,\n",
       "           0.022911,     0.01883,    0.018842,    0.018851,    0.017566,    0.016174,    0.015692,    0.015321,     0.01495,    0.014578,    0.013562,    0.013114,    0.013121,    0.013081,    0.011681,    0.011687,    0.011694,    0.011699,    0.011702,    0.011405,    0.010611,    0.010155,   0.0099831,\n",
       "           0.009811,   0.0096389,   0.0094667,   0.0092945,   0.0091222,     0.00895,   0.0086428,    0.007049,   0.0061252,   0.0055258,   0.0050506,   0.0045751,   0.0028468,   0.0026537,   0.0024605,   0.0022673,   0.0020741,   0.0018809,   0.0016876,   0.0014942,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[  0.0053444,   0.0053483,   0.0085089,    0.011246,    0.013677,    0.015931,    0.018046,    0.020063,    0.022032,    0.023796,    0.025713,    0.027651,    0.029302,    0.031209,    0.032832,    0.034742,    0.036637,    0.038516,    0.040444,    0.042327,    0.044191,    0.046015,    0.047944,\n",
       "           0.049761,    0.051731,    0.053832,    0.055696,    0.057724,    0.059843,    0.062064,    0.064214,    0.066384,    0.068156,    0.070437,    0.072679,    0.074797,     0.07722,     0.07895,    0.081534,    0.083554,    0.086342,    0.089136,    0.091447,    0.093182,    0.094531,    0.097071,\n",
       "           0.098996,     0.10107,      0.1036,     0.10512,     0.10694,     0.10947,     0.11112,     0.11345,     0.11586,     0.11773,     0.12006,     0.12173,       0.124,     0.12563,     0.12812,     0.13033,     0.13289,     0.13608,     0.13803,     0.14003,     0.14252,     0.14473,     0.14661,\n",
       "            0.14833,     0.15023,     0.15243,     0.15425,     0.15664,     0.15776,      0.1583,     0.15921,     0.16071,     0.16226,     0.16432,     0.16678,     0.16843,     0.17003,      0.1724,     0.17527,     0.17775,     0.17929,     0.18133,     0.18185,     0.18341,     0.18432,     0.18462,\n",
       "            0.18549,     0.18641,     0.18725,     0.18834,     0.18845,     0.18926,     0.19107,     0.19244,     0.19421,      0.1962,     0.19779,     0.19893,     0.19944,     0.20114,     0.20287,     0.20363,     0.20692,     0.20819,     0.20967,     0.21142,      0.2133,     0.21345,     0.21536,\n",
       "            0.21612,     0.21576,     0.21706,     0.21841,     0.21896,     0.22086,     0.22279,     0.22427,     0.22461,     0.22448,     0.22643,     0.22748,     0.22838,     0.22942,     0.23115,     0.23267,      0.2343,     0.23604,     0.23709,     0.23839,     0.23953,     0.24049,     0.24216,\n",
       "            0.24309,     0.24498,     0.24478,     0.24681,     0.24821,      0.2488,     0.25008,     0.25172,     0.25331,     0.25492,     0.25576,     0.25663,     0.25866,     0.25986,     0.26169,     0.26227,     0.26372,     0.26523,     0.26703,     0.27056,     0.26952,     0.27161,     0.27574,\n",
       "            0.27705,     0.27798,     0.27877,     0.27901,     0.28053,     0.28114,     0.28374,     0.28566,      0.2888,     0.28951,     0.29085,     0.29175,     0.29152,     0.29401,     0.29555,     0.29582,     0.29626,     0.29696,     0.29756,     0.29988,     0.30275,     0.30514,     0.30762,\n",
       "            0.31129,     0.31236,     0.31513,     0.31581,     0.31688,     0.31939,     0.32253,     0.32407,     0.32431,     0.32313,     0.32475,     0.32614,     0.32808,     0.32798,      0.3307,     0.33335,      0.3359,     0.33579,     0.33923,     0.33961,     0.33876,     0.34341,      0.3482,\n",
       "            0.35155,     0.35367,      0.3544,     0.35545,     0.35755,     0.35923,     0.36289,     0.36382,     0.36629,     0.37055,     0.37234,     0.37149,     0.37293,     0.37483,     0.37614,      0.3796,     0.38415,     0.38585,     0.38691,     0.38454,     0.38397,     0.38828,     0.39036,\n",
       "            0.39017,     0.39234,     0.39601,     0.39724,      0.3987,      0.4028,     0.40268,     0.40385,     0.40569,      0.4076,     0.41162,     0.41344,     0.41106,     0.41253,     0.41391,     0.41748,     0.41947,     0.42518,     0.42677,     0.43264,     0.43507,     0.43734,     0.43759,\n",
       "            0.43537,      0.4366,     0.43703,     0.43593,     0.43637,     0.43814,     0.44091,     0.44243,      0.4406,     0.44127,     0.44072,     0.44363,     0.44615,     0.44606,     0.44799,     0.45478,     0.45768,     0.45954,     0.45621,     0.46086,     0.46274,      0.4609,      0.4591,\n",
       "            0.46385,     0.46331,     0.46524,     0.47201,     0.47563,     0.47775,     0.47874,      0.4762,     0.48145,     0.48234,     0.48038,     0.47955,     0.48039,     0.47755,     0.48258,     0.49116,     0.49784,     0.50109,     0.50248,      0.5097,     0.51851,     0.52034,     0.52311,\n",
       "            0.52744,     0.53065,     0.52887,     0.53345,      0.5412,     0.54294,     0.54243,     0.54133,     0.54108,     0.54452,     0.54741,     0.55375,     0.55793,     0.56554,     0.57029,     0.58174,     0.57951,     0.58608,      0.5867,     0.59111,     0.59585,     0.59635,     0.59678,\n",
       "            0.59343,     0.60142,     0.60452,     0.60502,     0.61353,     0.61365,     0.61362,     0.61807,     0.61909,     0.61599,     0.62808,     0.63739,     0.64755,     0.67004,     0.67086,     0.67903,     0.67888,     0.69084,      0.6923,     0.69022,     0.67827,     0.66961,     0.65651,\n",
       "            0.64758,     0.64748,     0.65003,     0.65461,     0.66245,     0.67068,     0.66353,     0.68212,     0.67258,     0.66011,     0.67416,     0.67714,     0.66696,     0.67145,     0.66671,       0.655,     0.63933,     0.62843,     0.62251,     0.61777,     0.63223,     0.65491,     0.64895,\n",
       "            0.63779,     0.59739,     0.62284,     0.64262,     0.63349,     0.64959,     0.68306,      0.6777,     0.67234,     0.66698,     0.65031,     0.66301,     0.70081,     0.74925,     0.74571,     0.79489,     0.87163,     0.92509,     0.97126,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.97351,     0.97351,     0.95364,     0.93893,     0.92347,     0.90949,     0.89698,     0.88521,     0.87417,     0.85798,     0.84768,     0.83959,     0.82561,     0.81898,     0.80574,     0.79838,     0.78955,     0.78293,     0.77704,     0.77042,     0.76159,     0.75276,     0.74614,\n",
       "            0.73878,     0.73216,     0.72774,     0.72112,      0.7145,     0.70861,     0.70346,     0.69904,     0.69536,      0.6858,     0.68065,     0.67403,     0.66814,     0.66593,      0.6571,     0.65342,     0.64533,     0.64165,     0.64018,     0.63429,     0.62546,     0.61295,     0.60706,\n",
       "            0.59971,     0.59603,     0.59088,     0.58131,     0.57297,     0.56954,     0.56218,      0.5585,     0.55629,     0.54746,     0.54231,     0.53656,     0.53201,     0.52759,     0.52318,      0.5195,     0.51508,     0.51361,      0.5092,     0.50405,     0.49963,     0.49522,     0.48933,\n",
       "            0.48712,     0.48271,     0.47976,     0.47609,     0.47388,     0.46799,     0.46063,     0.45367,      0.4506,     0.44592,     0.44224,     0.43782,     0.43194,     0.42899,     0.42752,     0.42605,     0.42384,     0.41943,     0.41348,     0.40986,     0.40618,      0.4025,     0.39809,\n",
       "            0.39441,      0.3922,     0.38631,     0.38337,     0.38043,     0.37601,     0.37454,     0.37086,     0.36792,     0.36571,     0.36321,     0.35909,     0.35541,     0.35173,     0.35026,     0.34658,     0.34658,     0.34437,     0.34216,     0.34051,     0.33922,     0.33554,     0.33385,\n",
       "            0.32892,     0.32597,     0.32375,     0.32082,     0.31727,     0.31567,     0.31407,     0.31273,     0.30979,     0.30691,     0.30611,      0.3039,     0.30056,     0.29641,     0.29507,      0.2934,     0.29213,     0.29065,     0.28918,     0.28721,     0.28624,     0.28403,     0.28256,\n",
       "            0.28109,     0.28109,     0.27815,     0.27741,     0.27594,     0.27447,     0.27299,     0.27226,     0.27152,     0.27079,     0.26932,     0.26784,     0.26637,      0.2649,     0.26416,     0.26269,     0.26122,     0.26122,     0.26049,     0.26049,     0.25754,     0.25754,     0.25754,\n",
       "            0.25577,     0.25497,     0.25386,     0.25239,     0.25239,     0.25092,     0.25092,     0.25018,     0.25018,     0.24871,     0.24724,     0.24577,     0.24209,     0.24209,     0.24062,     0.23915,     0.23915,     0.23767,      0.2362,      0.2362,     0.23547,     0.23473,     0.23473,\n",
       "            0.23473,     0.23326,     0.23252,     0.23179,     0.23032,     0.22958,     0.22958,      0.2286,     0.22737,     0.22443,     0.22369,     0.22369,     0.22369,     0.22222,     0.22222,     0.22149,     0.22149,     0.21928,     0.21928,     0.21781,     0.21486,     0.21486,     0.21413,\n",
       "            0.21339,     0.21192,     0.20971,     0.20751,     0.20751,     0.20751,     0.20677,      0.2053,     0.20456,     0.20456,     0.20309,     0.20093,     0.20088,     0.20015,     0.19876,     0.19855,     0.19794,      0.1972,     0.19647,     0.19205,     0.18985,     0.18963,     0.18837,\n",
       "            0.18785,     0.18617,     0.18469,     0.18396,     0.18396,     0.18322,     0.18156,     0.17954,     0.17954,     0.17922,     0.17881,     0.17807,     0.17565,     0.17439,     0.17366,     0.17219,     0.17145,     0.17071,     0.16851,     0.16851,     0.16831,      0.1663,     0.16556,\n",
       "            0.16188,     0.16138,     0.15937,     0.15673,     0.15553,     0.15526,     0.15526,     0.15356,     0.15126,     0.15052,      0.1496,     0.14717,      0.1457,     0.14422,     0.14349,     0.14275,     0.14275,     0.14203,      0.1389,     0.13834,     0.13834,     0.13588,     0.13319,\n",
       "            0.13319,     0.13213,     0.13098,     0.13098,     0.12948,     0.12789,      0.1273,     0.12509,     0.12362,     0.12273,     0.12041,     0.11933,     0.11837,     0.11626,     0.11626,     0.11626,     0.11526,     0.11405,     0.11332,     0.11258,     0.11185,     0.11038,      0.1089,\n",
       "            0.10817,      0.1067,      0.1049,     0.10375,     0.10155,     0.10155,    0.098571,    0.098137,     0.09713,    0.096766,    0.095659,    0.094923,    0.093798,    0.092913,    0.092715,    0.091088,      0.0883,    0.087519,    0.086093,    0.085102,    0.083537,     0.07947,    0.077324,\n",
       "           0.076258,    0.075791,    0.073112,     0.07101,    0.069169,    0.067787,    0.066225,    0.066225,    0.065489,    0.062559,    0.060891,    0.059603,    0.059603,    0.058867,    0.057395,    0.056042,     0.05298,    0.052618,    0.051325,    0.049301,    0.046539,     0.04325,    0.040787,\n",
       "           0.038263,    0.036792,    0.034171,    0.033848,    0.033113,    0.033113,    0.030476,    0.028425,    0.027207,    0.025724,    0.025018,    0.024696,    0.023547,    0.022561,     0.02208,    0.019562,    0.018264,    0.017427,     0.01699,    0.015453,    0.014717,    0.013965,    0.012249,\n",
       "           0.011665,   0.0095659,   0.0095659,   0.0095659,   0.0089062,   0.0081891,   0.0079374,   0.0077481,   0.0075588,   0.0073695,   0.0068527,   0.0066225,   0.0066225,   0.0065982,   0.0058867,   0.0058867,   0.0058867,   0.0058867,   0.0058867,   0.0057351,   0.0053337,   0.0051035,   0.0050166,\n",
       "          0.0049297,   0.0048428,   0.0047559,   0.0046689,    0.004582,   0.0044951,   0.0043401,   0.0035369,    0.003072,   0.0027706,   0.0025317,   0.0022928,   0.0014254,   0.0013286,   0.0012318,    0.001135,   0.0010381,  0.00094131,  0.00084449,  0.00074767,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
       "fitness: np.float64(0.0863229376719806)\n",
       "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
       "maps: array([   0.073019])\n",
       "names: {0: 'pneumonia'}\n",
       "plot: True\n",
       "results_dict: {'metrics/precision(B)': np.float64(0.2955499990089747), 'metrics/recall(B)': np.float64(0.24061810154525387), 'metrics/mAP50(B)': np.float64(0.2060553902215879), 'metrics/mAP50-95(B)': np.float64(0.07301933183313533), 'fitness': np.float64(0.0863229376719806)}\n",
       "save_dir: WindowsPath('runs/detect/medical_imaging_v22')\n",
       "speed: {'preprocess': 0.5205246857587608, 'inference': 14.036788285342348, 'loss': 0.00019607997768603707, 'postprocess': 0.7217836922342833}\n",
       "task: 'detect'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(resume=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2621317e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa77ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027f95ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49525c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07abe829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.146 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.145  Python-3.13.1 torch-2.6.0+cu126 CUDA:0 (NVIDIA T400 4GB, 4096MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=YOLO_format_data/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=20, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=runs/detect/medical_imaging_v2/weights/best.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=medical_imaging_v3, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/detect, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\detect\\medical_imaging_v3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "WARNING \u001b[34m\u001b[1mAMP: \u001b[0mchecks failed . AMP training on NVIDIA T400 4GB GPU may cause NaN losses or zero-mAP results, so AMP will be disabled during training.\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 306.1181.6 MB/s, size: 59.3 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\w10195102\\Desktop\\PROJECTS\\ML_Projects\\Medical_Image_Classification\\YOLO_format_data\\labels\\train.cache... 20000 images, 14917 backgrounds, 0 corrupt: 100%|██████████| 20000/20000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 448.9233.3 MB/s, size: 57.1 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\w10195102\\Desktop\\PROJECTS\\ML_Projects\\Medical_Image_Classification\\YOLO_format_data\\labels\\val.cache... 6684 images, 5755 backgrounds, 0 corrupt: 100%|██████████| 6684/6684 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\medical_imaging_v3\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\medical_imaging_v3\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/20      3.84G      1.819       2.71      1.894         14        640: 100%|██████████| 1250/1250 [22:20<00:00,  1.07s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 209/209 [02:39<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6684       1359      0.185      0.238      0.124     0.0436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/20      3.84G      1.907      2.837      1.955         11        640: 100%|██████████| 1250/1250 [20:54<00:00,  1.00s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 209/209 [03:04<00:00,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6684       1359      0.299       0.25       0.18     0.0624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/20      3.83G      1.966      2.918      1.994         28        640: 100%|██████████| 1250/1250 [20:57<00:00,  1.01s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 209/209 [02:57<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6684       1359      0.224       0.18       0.12      0.041\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/20      3.84G      1.962      2.903      1.983          8        640: 100%|██████████| 1250/1250 [20:13<00:00,  1.03it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 209/209 [03:06<00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6684       1359      0.286      0.252      0.177     0.0588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/20      3.84G      1.959      2.851      1.988          4        640: 100%|██████████| 1250/1250 [22:05<00:00,  1.06s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 209/209 [02:58<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6684       1359      0.229       0.28      0.172     0.0623\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/20      3.84G       2.01      2.858      1.989          6        640:   4%|▍         | 52/1250 [00:53<20:36,  1.03s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m model = YOLO(\u001b[33m'\u001b[39m\u001b[33mruns/detect/medical_imaging_v2/weights/best.pt\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mYOLO_format_data/data.yaml\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mruns/detect\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmedical_imaging_v3\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w10195102\\Desktop\\PROJECTS\\ML_Projects\\.venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:797\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    794\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.trainer.model\n\u001b[32m    796\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer.hub_session = \u001b[38;5;28mself\u001b[39m.session  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m797\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[32m    799\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w10195102\\Desktop\\PROJECTS\\ML_Projects\\.venv\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:227\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    224\u001b[39m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w10195102\\Desktop\\PROJECTS\\ML_Projects\\.venv\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:406\u001b[39m, in \u001b[36mBaseTrainer._do_train\u001b[39m\u001b[34m(self, world_size)\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m autocast(\u001b[38;5;28mself\u001b[39m.amp):\n\u001b[32m    405\u001b[39m     batch = \u001b[38;5;28mself\u001b[39m.preprocess_batch(batch)\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m     loss, \u001b[38;5;28mself\u001b[39m.loss_items = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    407\u001b[39m     \u001b[38;5;28mself\u001b[39m.loss = loss.sum()\n\u001b[32m    408\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m RANK != -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w10195102\\Desktop\\PROJECTS\\ML_Projects\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w10195102\\Desktop\\PROJECTS\\ML_Projects\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w10195102\\Desktop\\PROJECTS\\ML_Projects\\.venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:137\u001b[39m, in \u001b[36mBaseModel.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[33;03mPerform forward pass of the model for either training or inference.\u001b[39;00m\n\u001b[32m    125\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    134\u001b[39m \u001b[33;03m    (torch.Tensor): Loss if x is a dict (training), or network predictions (inference).\u001b[39;00m\n\u001b[32m    135\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predict(x, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w10195102\\Desktop\\PROJECTS\\ML_Projects\\.venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:337\u001b[39m, in \u001b[36mBaseModel.loss\u001b[39m\u001b[34m(self, batch, preds)\u001b[39m\n\u001b[32m    334\u001b[39m     \u001b[38;5;28mself\u001b[39m.criterion = \u001b[38;5;28mself\u001b[39m.init_criterion()\n\u001b[32m    336\u001b[39m preds = \u001b[38;5;28mself\u001b[39m.forward(batch[\u001b[33m\"\u001b[39m\u001b[33mimg\u001b[39m\u001b[33m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m preds\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w10195102\\Desktop\\PROJECTS\\ML_Projects\\.venv\\Lib\\site-packages\\ultralytics\\utils\\loss.py:256\u001b[39m, in \u001b[36mv8DetectionLoss.__call__\u001b[39m\u001b[34m(self, preds, batch)\u001b[39m\n\u001b[32m    254\u001b[39m dtype = pred_scores.dtype\n\u001b[32m    255\u001b[39m batch_size = pred_scores.shape[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m imgsz = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m * \u001b[38;5;28mself\u001b[39m.stride[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# image size (h,w)\u001b[39;00m\n\u001b[32m    257\u001b[39m anchor_points, stride_tensor = make_anchors(feats, \u001b[38;5;28mself\u001b[39m.stride, \u001b[32m0.5\u001b[39m)\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# Targets\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model = YOLO('runs/detect/medical_imaging_v2/weights/best.pt')\n",
    "\n",
    "model.train(data='YOLO_format_data/data.yaml', epochs=20, project='runs/detect', name='medical_imaging_v3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fb4f1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('runs/detect/medical_imaging_v3/weights/last.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc39fecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.147 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.145  Python-3.13.1 torch-2.6.0+cu126 CUDA:0 (NVIDIA T400 4GB, 4096MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=YOLO_format_data/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=20, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=runs\\detect\\medical_imaging_v3\\weights\\last.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=medical_imaging_v3, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/detect, rect=False, resume=runs\\detect\\medical_imaging_v3\\weights\\last.pt, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\detect\\medical_imaging_v3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.0, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "WARNING \u001b[34m\u001b[1mAMP: \u001b[0mchecks failed . AMP training on NVIDIA T400 4GB GPU may cause NaN losses or zero-mAP results, so AMP will be disabled during training.\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 291.9125.7 MB/s, size: 59.3 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\w10195102\\Desktop\\PROJECTS\\ML_Projects\\Medical_Image_Classification\\YOLO_format_data\\labels\\train.cache... 20000 images, 14917 backgrounds, 0 corrupt: 100%|██████████| 20000/20000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 179.680.3 MB/s, size: 57.1 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\w10195102\\Desktop\\PROJECTS\\ML_Projects\\Medical_Image_Classification\\YOLO_format_data\\labels\\val.cache... 6684 images, 5755 backgrounds, 0 corrupt: 100%|██████████| 6684/6684 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\medical_imaging_v3\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Resuming training runs\\detect\\medical_imaging_v3\\weights\\last.pt from epoch 13 to 20 total epochs\n",
      "Closing dataloader mosaic\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\medical_imaging_v3\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/20      3.85G      1.805      2.656      1.872         14        640: 100%|██████████| 1250/1250 [20:40<00:00,  1.01it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 209/209 [02:56<00:00,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6684       1359      0.328      0.258       0.22     0.0744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/20      3.83G      1.784       2.59      1.856         11        640: 100%|██████████| 1250/1250 [21:32<00:00,  1.03s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 209/209 [03:05<00:00,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6684       1359        0.3      0.307      0.229     0.0809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/20      3.83G      1.767      2.584      1.846         28        640: 100%|██████████| 1250/1250 [20:18<00:00,  1.03it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 209/209 [02:44<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6684       1359      0.339      0.322      0.259     0.0909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/20      3.83G       1.75      2.544      1.834          8        640: 100%|██████████| 1250/1250 [19:49<00:00,  1.05it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 209/209 [03:06<00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6684       1359      0.322      0.298      0.249     0.0869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/20      3.83G      1.736      2.512      1.822          4        640: 100%|██████████| 1250/1250 [21:03<00:00,  1.01s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 209/209 [02:56<00:00,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6684       1359      0.296       0.32      0.238     0.0859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/20      3.83G      1.781      2.553      1.861          9        640:  77%|███████▋  | 963/1250 [16:05<04:47,  1.00s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w10195102\\Desktop\\PROJECTS\\ML_Projects\\.venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:797\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    794\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.trainer.model\n\u001b[32m    796\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer.hub_session = \u001b[38;5;28mself\u001b[39m.session  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m797\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[32m    799\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w10195102\\Desktop\\PROJECTS\\ML_Projects\\.venv\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:227\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    224\u001b[39m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w10195102\\Desktop\\PROJECTS\\ML_Projects\\.venv\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:435\u001b[39m, in \u001b[36mBaseTrainer._do_train\u001b[39m\u001b[34m(self, world_size)\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n\u001b[32m    434\u001b[39m     loss_length = \u001b[38;5;28mself\u001b[39m.tloss.shape[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.tloss.shape) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m     \u001b[43mpbar\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_description\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m%11s\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m%11.4g\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m        \u001b[49m\u001b[43m%\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43m+\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[32;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[33;43m.3g\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43mG\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# (GB) GPU memory util\u001b[39;49;00m\n\u001b[32m    440\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloss_length\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# losses\u001b[39;49;00m\n\u001b[32m    441\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# batch size, i.e. 8\u001b[39;49;00m\n\u001b[32m    442\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# imgsz, i.e 640\u001b[39;49;00m\n\u001b[32m    443\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m     \u001b[38;5;28mself\u001b[39m.run_callbacks(\u001b[33m\"\u001b[39m\u001b[33mon_batch_end\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    446\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.plots \u001b[38;5;129;01mand\u001b[39;00m ni \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.plot_idx:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w10195102\\Desktop\\PROJECTS\\ML_Projects\\.venv\\Lib\\site-packages\\tqdm\\std.py:1382\u001b[39m, in \u001b[36mtqdm.set_description\u001b[39m\u001b[34m(self, desc, refresh)\u001b[39m\n\u001b[32m   1379\u001b[39m     \u001b[38;5;28mself\u001b[39m._ema_miniters = EMA(\u001b[38;5;28mself\u001b[39m.smoothing)\n\u001b[32m   1380\u001b[39m     \u001b[38;5;28mself\u001b[39m.refresh()\n\u001b[32m-> \u001b[39m\u001b[32m1382\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mset_description\u001b[39m(\u001b[38;5;28mself\u001b[39m, desc=\u001b[38;5;28;01mNone\u001b[39;00m, refresh=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1383\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1384\u001b[39m \u001b[33;03m    Set/modify description of the progress bar.\u001b[39;00m\n\u001b[32m   1385\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1390\u001b[39m \u001b[33;03m        Forces refresh [default: True].\u001b[39;00m\n\u001b[32m   1391\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1392\u001b[39m     \u001b[38;5;28mself\u001b[39m.desc = desc + \u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m desc \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model.train(resume=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db71fac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae1634f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a57456a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982c0980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88f3e90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4682e21a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cba51b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a465dc55",
   "metadata": {},
   "source": [
    "Thanks for sharing your full data preparation pipeline — you've clearly put considerable effort into preparing the RSNA Pneumonia dataset for YOLOv8, and your code is mostly well-structured, functional, and thoughtful. Below is a detailed and structured review of your work, highlighting what you did well, what can be improved, and recommendations for enhancement from both engineering and machine learning perspectives.\n",
    "\n",
    "✅ What You Did Well\n",
    "1. Data Pipeline Structure & Clarity\n",
    "You follow a logical, modular structure — separating stages like DICOM to JPEG conversion, label processing, directory setup, and YAML generation.\n",
    "\n",
    "Clear and consistent naming conventions: Your use of variables like train_image_name_list, val_image_txt_list, etc., makes the code easy to follow.\n",
    "\n",
    "2. Proper DICOM Handling\n",
    "The use of pydicom to read .dcm files and conversion to .jpg with PIL.Image is well-implemented.\n",
    "\n",
    "You took care to extract pixel data correctly, which is essential when dealing with medical images.\n",
    "\n",
    "3. YOLO-Compatible Dataset Formatting\n",
    "You correctly created the directory structure expected by YOLOv8 (images/train, images/val, labels/train, labels/val).\n",
    "\n",
    "Proper formatting of annotation files in YOLO format (normalized x, y, w, h) is present.\n",
    "\n",
    "4. Basic Data Management\n",
    "File existence checks and directory creation with mkdir(..., exist_ok=True) are good defensive programming practices.\n",
    "\n",
    "Usage of Python's Path and os modules is appropriate for filesystem handling.\n",
    "\n",
    "⚠️ Areas of Improvement\n",
    "1. Code Efficiency and Readability\n",
    "🔸 Avoid Redundant Code\n",
    "There is a lot of duplicate code between training and validation loops for:\n",
    "\n",
    "Image conversion (dcm_to_jpg)\n",
    "\n",
    "Label processing\n",
    "\n",
    "Fix: Abstract these into reusable functions like:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "def process_images(dcm_paths, save_dir, name_list):\n",
    "    for idx, dcm in enumerate(dcm_paths):\n",
    "        save_path = os.path.join(save_dir, name_list[idx])\n",
    "        dcm_to_jpg(dcm, save_path)\n",
    "🔸 Hardcoding Image Size\n",
    "You hardcoded normalization with 1024:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "x1 = bbox_data.iloc[x, 1].item() / 1024\n",
    "However, DICOM images in the dataset may not all be 1024×1024.\n",
    "\n",
    "Fix:\n",
    "Dynamically extract the image shape:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "dcm_data = dcmread(path)\n",
    "height, width = dcm_data.pixel_array.shape\n",
    "x1 = bbox_data.iloc[x, 1].item() / width\n",
    "2. Missing Negative Class (No Pneumonia) Handling\n",
    "Your label generation ignores negative samples (else: pass). These are crucial for object detection models to learn what not to detect.\n",
    "\n",
    "Fix: Still write an empty .txt file for such images:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "else:\n",
    "    open(labels_train_txt, 'w').close()\n",
    "YOLO expects a label file per image — even if it's empty.\n",
    "\n",
    "3. Better Metadata and Logging\n",
    "You’re currently not using logging for debugging or performance profiling.\n",
    "\n",
    "Fix: Use logging module to add insight during dataset prep:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "logging.info(f'Processing image {index + 1}/{len(yolo_train_path)}')\n",
    "4. Image Normalization or Contrast Handling\n",
    "Medical images like DICOMs may have contrast issues. A basic .jpg conversion may cause loss of important clinical features.\n",
    "\n",
    "Suggestion: Apply contrast enhancement / histogram equalization using cv2.equalizeHist() or SimpleITK to enhance image quality before saving to .jpg.\n",
    "\n",
    "🧠 Deeper Machine Learning Considerations\n",
    "1. Data Stratification\n",
    "Your split is purely random ([:20000], [20000:]), but there may be class imbalance (i.e., many images without pneumonia). Ensure both train and val sets maintain a similar positive/negative class distribution.\n",
    "\n",
    "Fix: Stratified splitting:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_pos = label_csv[label_csv['Target'] == 1]\n",
    "df_neg = label_csv[label_csv['Target'] == 0]\n",
    "\n",
    "train_ids, val_ids = train_test_split(df_pos['patientId'].unique(), stratify=df_pos['Target'], test_size=0.25)\n",
    "2. Image Preprocessing and Augmentation\n",
    "Currently, images are not preprocessed for brightness, noise, or other modalities. Consider:\n",
    "\n",
    "Applying clahe (Contrast Limited Adaptive Histogram Equalization)\n",
    "\n",
    "Rescaling to a standard resolution (YOLOv8 can handle this at training, but consistency helps)\n",
    "\n",
    "Also, consider adding augmentations like:\n",
    "\n",
    "Random flips\n",
    "\n",
    "Rotation\n",
    "\n",
    "Brightness adjustment\n",
    "\n",
    "These can be done during training using YOLOv8's in-built augmentation or via Albumentations.\n",
    "\n",
    "📦 Final Thoughts: Your Strengths & Where to Improve\n",
    "✅ Strengths\t⚠️ Areas to Improve\n",
    "Clean structure & use of Pathlib\tDuplicate code — refactor for modularity\n",
    "Correct YOLO formatting & dir structure\tLabel processing needs better handling of negatives\n",
    "Good use of CSV + Pandas\tHardcoded image shape — should be dynamic\n",
    "Accurate data.yaml creation\tNo data stratification or class balance consideration\n",
    "Working DICOM to JPEG conversion\tNo image contrast enhancement or medical image preprocessing\n",
    "\n",
    "⭐ Recommendations\n",
    "Refactor repetitive blocks into functions.\n",
    "\n",
    "Handle negative (no pneumonia) examples more explicitly.\n",
    "\n",
    "Include image shape extraction dynamically.\n",
    "\n",
    "Log key steps and potential warnings.\n",
    "\n",
    "Apply basic preprocessing to enhance DICOM image contrast.\n",
    "\n",
    "Optionally, verify the YOLOv8 training quality via mAP for early feedback.\n",
    "\n",
    "Let me know if you'd like help refactoring this into cleaner reusable functions or adding the next stage (training, evaluation, visualization, or inference)!\n",
    "\n",
    "\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "\n",
    "You said:\n",
    "i was expecting u to give other feedback like lack of visualizing data in differnt types of graph..... and no data cleaning and stugg\n",
    "\n",
    "\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "\n",
    "ChatGPT said:\n",
    "Great follow-up — you're absolutely right to expect feedback beyond the mechanical and code-focused aspects. Data science, especially in medical imaging, isn't just about getting the data into the right format, but also about understanding, cleaning, and validating it before training. Let’s dive deeper into what’s missing or underexplored in your pipeline from a data science, ML, and EDA (Exploratory Data Analysis) perspective.\n",
    "\n",
    "🧠 High-Level Gaps in Your Pipeline\n",
    "1. No Data Exploration or Visualization\n",
    "You jumped directly into converting and formatting data, but without knowing what you’re working with, you risk feeding your model noisy, biased, or incomplete data.\n",
    "\n",
    "🚩What You Missed:\n",
    "Distribution of bounding boxes (sizes, locations)\n",
    "\n",
    "Image modality metadata (some images might be corrupted, noisy, or extremely dark/bright)\n",
    "\n",
    "Class imbalance (how many pneumonia vs non-pneumonia samples?)\n",
    "\n",
    "✅ What You Could Have Done:\n",
    "Visualizations that would’ve helped:\n",
    "\n",
    "Bar chart of pneumonia vs. non-pneumonia cases\n",
    "\n",
    "Histogram of bounding box widths/heights\n",
    "\n",
    "Sample images with bounding boxes plotted\n",
    "\n",
    "Image intensity histograms to inspect contrast/brightness variations\n",
    "\n",
    "Patient age or gender distribution, if metadata is available\n",
    "\n",
    "Example:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "label_csv['Target'].value_counts().plot(kind='bar')\n",
    "plt.title('Pneumonia vs Normal Distribution')\n",
    "plt.xticks([0, 1], ['No Pneumonia', 'Pneumonia'], rotation=0)\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.show()\n",
    "2. No Data Cleaning or Sanity Checks\n",
    "🚩What You Missed:\n",
    "Duplicated entries in the CSV? (can happen in large medical datasets)\n",
    "\n",
    "Missing or corrupted images?\n",
    "\n",
    "Inconsistent label coordinates (e.g., negative width/height, out-of-bound values)?\n",
    "\n",
    "Check if all .dcm files are referenced in the CSV\n",
    "\n",
    "✅ Checks You Could Include:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Check for missing values\n",
    "print(label_csv.isnull().sum())\n",
    "\n",
    "# Ensure coordinates are valid\n",
    "assert (label_csv['width'] >= 0).all()\n",
    "assert (label_csv['height'] >= 0).all()\n",
    "\n",
    "# Remove duplicate rows\n",
    "label_csv = label_csv.drop_duplicates()\n",
    "3. No Sanity Check on Label Mapping\n",
    "You assume:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "label_class_id = bbox_data.iloc[0, 5].item()\n",
    "f.write(f'{label_class_id-1} ...')\n",
    "But this only works because pneumonia is the only class and is labeled as 1 — what if there were more classes later? Or different labeling styles?\n",
    "\n",
    "✅ More robust:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "class_map = {1: 0}  # map pneumonia to class 0\n",
    "class_id = class_map.get(label_class_id, -1)\n",
    "4. No Summary Stats / Dataset Profiling\n",
    "A simple summary table could give you insight like:\n",
    "\n",
    "Avg number of bboxes per pneumonia case\n",
    "\n",
    "% of images that have multiple boxes\n",
    "\n",
    "Avg width/height ratio\n",
    "\n",
    "These are super helpful for debugging model performance later.\n",
    "\n",
    "Example:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "pneumonia_cases = label_csv[label_csv['Target'] == 1]\n",
    "avg_boxes = pneumonia_cases.groupby('patientId').size().mean()\n",
    "print(f\"Average boxes per pneumonia image: {avg_boxes}\")\n",
    "5. No Verification After Conversion\n",
    "There’s no check that:\n",
    "\n",
    "All .jpg images were saved successfully\n",
    "\n",
    "All labels have a corresponding image\n",
    "\n",
    "Images and labels are correctly matched by filename\n",
    "\n",
    "Danger: Any mismatch = silent failure in YOLO training\n",
    "\n",
    "✅ Fix:\n",
    "Write a post-process checker:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "img_files = set(f.stem for f in Path('YOLO_format_data/images/train').glob('*.jpg'))\n",
    "label_files = set(f.stem for f in Path('YOLO_format_data/labels/train').glob('*.txt'))\n",
    "missing_labels = img_files - label_files\n",
    "print(f'Images with missing labels: {len(missing_labels)}')\n",
    "6. No Metadata Use (if available)\n",
    "The RSNA dataset comes with some metadata per image:\n",
    "\n",
    "Patient ID, age, gender, view position\n",
    "You didn’t use or explore any of it.\n",
    "\n",
    "✅ Could Use For:\n",
    "Stratified splitting based on demographics\n",
    "\n",
    "Bias detection\n",
    "\n",
    "Error analysis after training\n",
    "\n",
    "📌 Summary of What to Improve (Broader Feedback)\n",
    "Category\tGap\tSuggested Action\n",
    "EDA\tNo graphs, plots, or data exploration\tUse Matplotlib/Seaborn to visualize distributions, sizes, samples\n",
    "Data Cleaning\tNo checks for missing/corrupted data, duplicates, bad coordinates\tAdd sanity checks, remove bad samples\n",
    "Class Balance\tAssumed balanced split, no stratification\tCount and balance samples explicitly\n",
    "Data Verification\tNo check that every image has a label and vice versa\tPost-process validation code\n",
    "Metadata Use\tIgnored demographic and acquisition metadata\tExplore metadata for patterns, fairness, or stratified split\n",
    "Bounding Box Quality\tDidn't visualize any sample with bounding box\tPlot a few random images + boxes for confirmation\n",
    "Image Preprocessing\tNo normalization, contrast enhancement\tEnhance contrast using cv2 or SimpleITK\n",
    "Automation & Reusability\tLots of duplicate code across train/val sections\tRefactor into reusable functions\n",
    "\n",
    "🎯 Final Suggestion\n",
    "Before training, it’s a good idea to create a visual data report or even a Jupyter notebook for:\n",
    "\n",
    "EDA graphs\n",
    "\n",
    "Sample visualizations (DICOM + boxes)\n",
    "\n",
    "Class balance summary\n",
    "\n",
    "Bounding box heatmaps (using seaborn or matplotlib)\n",
    "\n",
    "Let me know if you want help creating these EDA plots or adding sanity checks — I can write some ready-to-use code for that."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
